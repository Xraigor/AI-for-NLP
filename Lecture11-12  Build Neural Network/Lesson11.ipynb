{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络 Neural Network\n",
    "    Python + Numpy手动实现tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关重要 References\n",
    "+ [**Deep Learing Book**](https://github.com/Artificial-Intelligence-for-NLP/References/blob/master/AI%20%26%20Machine%20Learning/DeepLearningBook.pdf)\n",
    "+ [**Stanford CS231n Turtorial**](https://cs231n.github.io/neural-networks-3/)\n",
    "+ [**Hands-on Tensorflow**](https://github.com/Artificial-Intelligence-for-NLP/References/blob/master/AI%20%26%20Machine%20Learning/Hands.On.TensorFlow.pdf)\n",
    "+ [**Backpropagation from Hinton**](https://github.com/Artificial-Intelligence-for-NLP/References/blob/master/AI%20%26%20Machine%20Learning/backprop_hinton.pdf)\n",
    "\n",
    "+ [**Monmentum for Optimizer**](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程 Slides\n",
    "> [Lesson-11.pdf](https://trello-attachments.s3.amazonaws.com/5c6bbba9b0624d5d97727d93/5d0e423edff47b22fee0c0d3/ca1012f362d3ee0b601fb95a83e2e144/Lesson-11-Neural-Networks.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Perception 感知器 非线性变化计算\n",
    "    Back propagation 反向传播\n",
    "    Activation 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning 迁移学习\n",
    "    从A problem 到B problem，如何改变深度学习框架适应新问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Network(AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH0: loss=6040.000, Gradient=-1551.000. W=-7.224\n",
      "EPOCH1: loss=4897.340, Gradient=-1395.900. W=-6.527\n",
      "EPOCH2: loss=3971.785, Gradient=-1256.310. W=-5.898\n",
      "EPOCH3: loss=3222.085, Gradient=-1130.679. W=-5.333\n",
      "EPOCH4: loss=2614.828, Gradient=-1017.611. W=-4.824\n",
      "EPOCH5: loss=2122.951, Gradient=-915.850. W=-4.366\n",
      "EPOCH6: loss=1724.529, Gradient=-824.265. W=-3.954\n",
      "EPOCH7: loss=1401.808, Gradient=-741.838. W=-3.583\n",
      "EPOCH8: loss=1140.404, Gradient=-667.655. W=-3.249\n",
      "EPOCH9: loss=928.667, Gradient=-600.889. W=-2.949\n",
      "EPOCH10: loss=757.160, Gradient=-540.800. W=-2.679\n",
      "EPOCH11: loss=618.239, Gradient=-486.720. W=-2.435\n",
      "EPOCH12: loss=505.713, Gradient=-438.048. W=-2.216\n",
      "EPOCH13: loss=414.567, Gradient=-394.243. W=-2.019\n",
      "EPOCH14: loss=340.739, Gradient=-354.819. W=-1.842\n",
      "EPOCH15: loss=280.938, Gradient=-319.337. W=-1.682\n",
      "EPOCH16: loss=232.499, Gradient=-287.403. W=-1.538\n",
      "EPOCH17: loss=193.264, Gradient=-258.663. W=-1.409\n",
      "EPOCH18: loss=161.483, Gradient=-232.797. W=-1.293\n",
      "EPOCH19: loss=135.741, Gradient=-209.517. W=-1.188\n",
      "EPOCH20: loss=114.890, Gradient=-188.565. W=-1.094\n",
      "EPOCH21: loss=98.000, Gradient=-169.709. W=-1.009\n",
      "EPOCH22: loss=84.320, Gradient=-152.738. W=-0.932\n",
      "EPOCH23: loss=73.238, Gradient=-137.464. W=-0.864\n",
      "EPOCH24: loss=64.263, Gradient=-123.718. W=-0.802\n",
      "EPOCH25: loss=56.992, Gradient=-111.346. W=-0.746\n",
      "EPOCH26: loss=51.103, Gradient=-100.211. W=-0.696\n",
      "EPOCH27: loss=46.333, Gradient=-90.190. W=-0.651\n",
      "EPOCH28: loss=42.469, Gradient=-81.171. W=-0.610\n",
      "EPOCH29: loss=39.340, Gradient=-73.054. W=-0.574\n",
      "EPOCH30: loss=36.805, Gradient=-65.749. W=-0.541\n",
      "EPOCH31: loss=34.751, Gradient=-59.174. W=-0.511\n",
      "EPOCH32: loss=33.088, Gradient=-53.256. W=-0.485\n",
      "EPOCH33: loss=31.741, Gradient=-47.931. W=-0.461\n",
      "EPOCH34: loss=30.650, Gradient=-43.138. W=-0.439\n",
      "EPOCH35: loss=29.766, Gradient=-38.824. W=-0.420\n",
      "EPOCH36: loss=29.050, Gradient=-34.942. W=-0.402\n",
      "EPOCH37: loss=28.470, Gradient=-31.447. W=-0.387\n",
      "EPOCH38: loss=28.000, Gradient=-28.303. W=-0.372\n",
      "EPOCH39: loss=27.620, Gradient=-25.472. W=-0.360\n",
      "EPOCH40: loss=27.311, Gradient=-22.925. W=-0.348\n",
      "EPOCH41: loss=27.062, Gradient=-20.633. W=-0.338\n",
      "EPOCH42: loss=26.860, Gradient=-18.569. W=-0.329\n",
      "EPOCH43: loss=26.696, Gradient=-16.712. W=-0.320\n",
      "EPOCH44: loss=26.563, Gradient=-15.041. W=-0.313\n",
      "EPOCH45: loss=26.456, Gradient=-13.537. W=-0.306\n",
      "EPOCH46: loss=26.369, Gradient=-12.183. W=-0.300\n",
      "EPOCH47: loss=26.298, Gradient=-10.965. W=-0.294\n",
      "EPOCH48: loss=26.241, Gradient=-9.869. W=-0.289\n",
      "EPOCH49: loss=26.195, Gradient=-8.882. W=-0.285\n",
      "EPOCH50: loss=26.157, Gradient=-7.994. W=-0.281\n",
      "EPOCH51: loss=26.127, Gradient=-7.194. W=-0.277\n",
      "EPOCH52: loss=26.102, Gradient=-6.475. W=-0.274\n",
      "EPOCH53: loss=26.082, Gradient=-5.827. W=-0.271\n",
      "EPOCH54: loss=26.066, Gradient=-5.245. W=-0.269\n",
      "EPOCH55: loss=26.053, Gradient=-4.720. W=-0.266\n",
      "EPOCH56: loss=26.043, Gradient=-4.248. W=-0.264\n",
      "EPOCH57: loss=26.034, Gradient=-3.823. W=-0.262\n",
      "EPOCH58: loss=26.027, Gradient=-3.441. W=-0.260\n",
      "EPOCH59: loss=26.021, Gradient=-3.097. W=-0.259\n",
      "EPOCH60: loss=26.017, Gradient=-2.787. W=-0.258\n",
      "EPOCH61: loss=26.013, Gradient=-2.508. W=-0.256\n",
      "EPOCH62: loss=26.010, Gradient=-2.258. W=-0.255\n",
      "EPOCH63: loss=26.008, Gradient=-2.032. W=-0.254\n",
      "EPOCH64: loss=26.006, Gradient=-1.829. W=-0.253\n",
      "EPOCH65: loss=26.004, Gradient=-1.646. W=-0.252\n",
      "EPOCH66: loss=26.003, Gradient=-1.481. W=-0.252\n",
      "EPOCH67: loss=26.002, Gradient=-1.333. W=-0.251\n",
      "EPOCH68: loss=26.001, Gradient=-1.200. W=-0.250\n",
      "EPOCH69: loss=26.000, Gradient=-1.080. W=-0.250\n",
      "EPOCH70: loss=26.000, Gradient=-0.972. W=-0.249\n",
      "EPOCH71: loss=25.999, Gradient=-0.875. W=-0.249\n",
      "EPOCH72: loss=25.999, Gradient=-0.787. W=-0.249\n",
      "EPOCH73: loss=25.999, Gradient=-0.708. W=-0.248\n",
      "EPOCH74: loss=25.999, Gradient=-0.638. W=-0.248\n",
      "EPOCH75: loss=25.998, Gradient=-0.574. W=-0.248\n",
      "EPOCH76: loss=25.998, Gradient=-0.516. W=-0.247\n",
      "EPOCH77: loss=25.998, Gradient=-0.465. W=-0.247\n",
      "EPOCH78: loss=25.998, Gradient=-0.418. W=-0.247\n",
      "EPOCH79: loss=25.998, Gradient=-0.377. W=-0.247\n",
      "EPOCH80: loss=25.998, Gradient=-0.339. W=-0.247\n",
      "EPOCH81: loss=25.998, Gradient=-0.305. W=-0.246\n",
      "EPOCH82: loss=25.998, Gradient=-0.274. W=-0.246\n",
      "EPOCH83: loss=25.998, Gradient=-0.247. W=-0.246\n",
      "EPOCH84: loss=25.998, Gradient=-0.222. W=-0.246\n",
      "EPOCH85: loss=25.998, Gradient=-0.200. W=-0.246\n",
      "EPOCH86: loss=25.998, Gradient=-0.180. W=-0.246\n",
      "EPOCH87: loss=25.998, Gradient=-0.162. W=-0.246\n",
      "EPOCH88: loss=25.998, Gradient=-0.146. W=-0.246\n",
      "EPOCH89: loss=25.998, Gradient=-0.131. W=-0.246\n",
      "EPOCH90: loss=25.998, Gradient=-0.118. W=-0.246\n",
      "EPOCH91: loss=25.998, Gradient=-0.106. W=-0.245\n",
      "EPOCH92: loss=25.998, Gradient=-0.096. W=-0.245\n",
      "EPOCH93: loss=25.998, Gradient=-0.086. W=-0.245\n",
      "EPOCH94: loss=25.998, Gradient=-0.078. W=-0.245\n",
      "EPOCH95: loss=25.998, Gradient=-0.070. W=-0.245\n",
      "EPOCH96: loss=25.998, Gradient=-0.063. W=-0.245\n",
      "EPOCH97: loss=25.998, Gradient=-0.057. W=-0.245\n",
      "EPOCH98: loss=25.998, Gradient=-0.051. W=-0.245\n",
      "EPOCH99: loss=25.998, Gradient=-0.046. W=-0.245\n",
      "EPOCH100: loss=25.998, Gradient=-0.041. W=-0.245\n",
      "EPOCH101: loss=25.998, Gradient=-0.037. W=-0.245\n",
      "EPOCH102: loss=25.998, Gradient=-0.033. W=-0.245\n",
      "EPOCH103: loss=25.998, Gradient=-0.030. W=-0.245\n",
      "EPOCH104: loss=25.998, Gradient=-0.027. W=-0.245\n",
      "EPOCH105: loss=25.998, Gradient=-0.024. W=-0.245\n",
      "EPOCH106: loss=25.998, Gradient=-0.022. W=-0.245\n",
      "EPOCH107: loss=25.998, Gradient=-0.020. W=-0.245\n",
      "EPOCH108: loss=25.998, Gradient=-0.018. W=-0.245\n",
      "EPOCH109: loss=25.998, Gradient=-0.016. W=-0.245\n",
      "EPOCH110: loss=25.998, Gradient=-0.014. W=-0.245\n",
      "EPOCH111: loss=25.998, Gradient=-0.013. W=-0.245\n",
      "EPOCH112: loss=25.998, Gradient=-0.012. W=-0.245\n",
      "EPOCH113: loss=25.998, Gradient=-0.010. W=-0.245\n",
      "EPOCH114: loss=25.998, Gradient=-0.009. W=-0.245\n",
      "EPOCH115: loss=25.998, Gradient=-0.008. W=-0.245\n",
      "EPOCH116: loss=25.998, Gradient=-0.008. W=-0.245\n",
      "EPOCH117: loss=25.998, Gradient=-0.007. W=-0.245\n",
      "EPOCH118: loss=25.998, Gradient=-0.006. W=-0.245\n",
      "EPOCH119: loss=25.998, Gradient=-0.006. W=-0.245\n",
      "EPOCH120: loss=25.998, Gradient=-0.005. W=-0.245\n",
      "EPOCH121: loss=25.998, Gradient=-0.005. W=-0.245\n",
      "EPOCH122: loss=25.998, Gradient=-0.004. W=-0.245\n",
      "EPOCH123: loss=25.998, Gradient=-0.004. W=-0.245\n",
      "EPOCH124: loss=25.998, Gradient=-0.003. W=-0.245\n",
      "EPOCH125: loss=25.998, Gradient=-0.003. W=-0.245\n",
      "EPOCH126: loss=25.998, Gradient=-0.003. W=-0.245\n",
      "EPOCH127: loss=25.998, Gradient=-0.002. W=-0.245\n",
      "EPOCH128: loss=25.998, Gradient=-0.002. W=-0.245\n",
      "EPOCH129: loss=25.998, Gradient=-0.002. W=-0.245\n",
      "EPOCH130: loss=25.998, Gradient=-0.002. W=-0.245\n",
      "EPOCH131: loss=25.998, Gradient=-0.002. W=-0.245\n",
      "EPOCH132: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH133: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH134: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH135: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH136: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH137: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH138: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH139: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH140: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH141: loss=25.998, Gradient=-0.001. W=-0.245\n",
      "EPOCH142: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH143: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH144: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH145: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH146: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH147: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH148: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH149: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH150: loss=25.998, Gradient=-0.000. W=-0.245\n",
      "EPOCH151: loss=25.998, Gradient=-0.000. W=-0.245\n"
     ]
    }
   ],
   "source": [
    "def f(w): ## loss function\n",
    "    return 100 * w **2 + 49 * w + 32\n",
    "\n",
    "def df(w):\n",
    "    return 200 * w + 49\n",
    "\n",
    "old_w = float('inf')\n",
    "w = random.randint(-100, 100)\n",
    "learning_rate = 0.001 ## 学习率不能太高\n",
    "\n",
    "epochs = 0 ## 一次activate + loss返回为一次epoch\n",
    "alpha = 0.5\n",
    "beta = 1 - alpha\n",
    "\n",
    "previous_w = None\n",
    "\n",
    "while abs(w - old_w) > 1.0e-7:\n",
    "    cost = f(w)\n",
    "    grad_w = df(w)\n",
    "    \n",
    "    old_w = w\n",
    "    # adaptive learning rate\n",
    "    ## first, we could set the learning rate as the function of grad_w\n",
    "    ## secend, we could set the learning rate as the result of current grad and previous_w\n",
    "    # momentum to record the last w, use their vector sum\n",
    "#     if previous_w:\n",
    "#         momentum = beta * previous_w\n",
    "#     else:\n",
    "#         momentum = 0\n",
    "    w += alpha * ( -1 * learning_rate * grad_w) # optimizer                                                                                                                                                                                                  \n",
    "    \n",
    "    print('EPOCH{}: loss={:.3f}, Gradient={:.3f}. W={:.3f}'.format(epochs, cost, grad_w, w))\n",
    "    epochs +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network vs Other ML Models \n",
    "    • Powerful\n",
    "    • Generalization\n",
    "    • Versatile\n",
    "    • Data Feeding "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
